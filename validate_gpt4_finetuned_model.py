#!/usr/bin/env python3
import os
import json
import time
import random
from datetime import datetime
from openai import OpenAI
from tqdm import tqdm
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
import numpy as np
from dotenv import load_dotenv

# í™˜ê²½ë³€ìˆ˜ ë¡œë“œ
load_dotenv()

# OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))

def load_validation_data(file_path, sample_size=100):
    """Validation ë°ì´í„° ë¡œë“œ ë° ìƒ˜í”Œë§"""
    with open(file_path, 'r', encoding='utf-8') as f:
        data = [json.loads(line) for line in f]
    
    # ë¬´ì‘ìœ„ë¡œ 100ê°œ ìƒ˜í”Œë§
    if len(data) > sample_size:
        data = random.sample(data, sample_size)
    
    return data

def load_model_info():
    """íŒŒì¸íŠœë‹ëœ ëª¨ë¸ ì •ë³´ ë¡œë“œ"""
    with open('data/model_info.json', 'r', encoding='utf-8') as f:
        return json.load(f)

def evaluate_translation(model_id, test_sample):
    """ë‹¨ì¼ ë²ˆì—­ í‰ê°€"""
    try:
        korean_text = test_sample['messages'][0]['content']
        expected_sign = test_sample['messages'][1]['content']
        
        # ëª¨ë¸ë¡œ ë²ˆì—­
        response = client.chat.completions.create(
            model=model_id,
            messages=[
                {"role": "user", "content": korean_text}
            ],
            max_tokens=200,
            temperature=0.1
        )
        
        predicted_sign = response.choices[0].message.content.strip()
        
        return {
            'korean': korean_text,
            'expected': expected_sign,
            'predicted': predicted_sign,
            'exact_match': predicted_sign == expected_sign
        }
        
    except Exception as e:
        print(f"ì˜¤ë¥˜ ë°œìƒ: {e}")
        return None

def calculate_token_overlap(expected, predicted):
    """í† í° ë‹¨ìœ„ ì¼ì¹˜ìœ¨ ê³„ì‚°"""
    expected_tokens = set(expected.split('+'))
    predicted_tokens = set(predicted.split('+'))
    
    if not expected_tokens:
        return 0.0
        
    intersection = expected_tokens.intersection(predicted_tokens)
    return len(intersection) / len(expected_tokens)

def evaluate_model(model_id, validation_data):
    """ëª¨ë¸ ì „ì²´ í‰ê°€"""
    results = []
    exact_matches = 0
    token_overlaps = []
    
    print(f"\nğŸ”„ ëª¨ë¸ í‰ê°€ ì‹œì‘: {model_id}")
    print(f"ğŸ“Š í‰ê°€í•  ë¬¸ì¥ ìˆ˜: {len(validation_data)}")
    
    # ì§„í–‰ë¥  í‘œì‹œ
    for sample in tqdm(validation_data, desc="í‰ê°€ ì§„í–‰"):
        result = evaluate_translation(model_id, sample)
        
        if result:
            results.append(result)
            
            if result['exact_match']:
                exact_matches += 1
            
            # í† í° ì˜¤ë²„ë© ê³„ì‚°
            overlap = calculate_token_overlap(result['expected'], result['predicted'])
            token_overlaps.append(overlap)
        
        # API ì œí•œ ë°©ì§€ë¥¼ ìœ„í•œ ëŒ€ê¸°
        time.sleep(0.1)
    
    # í‰ê°€ ì§€í‘œ ê³„ì‚°
    accuracy = exact_matches / len(results) if results else 0
    avg_token_overlap = np.mean(token_overlaps) if token_overlaps else 0
    
    return {
        'model_id': model_id,
        'total_samples': len(validation_data),
        'evaluated_samples': len(results),
        'exact_match_accuracy': accuracy,
        'avg_token_overlap': avg_token_overlap,
        'results': results
    }

def print_evaluation_summary(evaluation):
    """í‰ê°€ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
    print("\n" + "="*60)
    print("ğŸ“Š í‰ê°€ ê²°ê³¼ ìš”ì•½")
    print("="*60)
    print(f"ëª¨ë¸ ID: {evaluation['model_id']}")
    print(f"í‰ê°€ ë¬¸ì¥ ìˆ˜: {evaluation['evaluated_samples']}/{evaluation['total_samples']}")
    print(f"ì •í™•í•œ ì¼ì¹˜ìœ¨: {evaluation['exact_match_accuracy']:.2%}")
    print(f"í‰ê·  í† í° ì¼ì¹˜ìœ¨: {evaluation['avg_token_overlap']:.2%}")
    
    # ìƒ˜í”Œ ê²°ê³¼ ì¶œë ¥
    print("\nğŸ“ ìƒ˜í”Œ ê²°ê³¼ (ì²˜ìŒ 5ê°œ):")
    print("-"*60)
    
    for i, result in enumerate(evaluation['results'][:5], 1):
        print(f"\nì˜ˆì‹œ {i}:")
        print(f"í•œêµ­ì–´: {result['korean']}")
        print(f"ì •ë‹µ: {result['expected']}")
        print(f"ì˜ˆì¸¡: {result['predicted']}")
        print(f"ì¼ì¹˜: {'âœ…' if result['exact_match'] else 'âŒ'}")
        print(f"í† í° ì¼ì¹˜ìœ¨: {calculate_token_overlap(result['expected'], result['predicted']):.2%}")

def save_evaluation_results(evaluation):
    """í‰ê°€ ê²°ê³¼ ì €ì¥"""
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file = f"gpt4_validation_results_{timestamp}.json"
    
    with open(output_file, 'w', encoding='utf-8') as f:
        json.dump(evaluation, f, ensure_ascii=False, indent=2)
    
    print(f"\nğŸ’¾ í‰ê°€ ê²°ê³¼ ì €ì¥ë¨: {output_file}")
    
    # ê°„ë‹¨í•œ ìš”ì•½ CSV íŒŒì¼ë„ ìƒì„±
    summary_file = f"gpt4_validation_summary_{timestamp}.csv"
    with open(summary_file, 'w', encoding='utf-8') as f:
        f.write("ëª¨ë¸ID,í‰ê°€ë¬¸ì¥ìˆ˜,ì •í™•ì¼ì¹˜ìœ¨,í‰ê· í† í°ì¼ì¹˜ìœ¨\n")
        f.write(f"{evaluation['model_id']},{evaluation['evaluated_samples']},")
        f.write(f"{evaluation['exact_match_accuracy']:.4f},{evaluation['avg_token_overlap']:.4f}\n")
    
    print(f"ğŸ“Š ìš”ì•½ íŒŒì¼ ì €ì¥ë¨: {summary_file}")

def main():
    # API í‚¤ í™•ì¸
    if not os.getenv('OPENAI_API_KEY'):
        print("âŒ OPENAI_API_KEY í™˜ê²½ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”!")
        return
    
    # ëª¨ë¸ ì •ë³´ ë¡œë“œ
    try:
        model_info = load_model_info()
        model_id = model_info['fine_tuned_model_id']
        print(f"âœ… íŒŒì¸íŠœë‹ ëª¨ë¸ ë¡œë“œë¨: {model_id}")
    except Exception as e:
        print(f"âŒ ëª¨ë¸ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # Validation ë°ì´í„° ë¡œë“œ (100ê°œ ìƒ˜í”Œ)
    try:
        validation_data = load_validation_data('data/validation_1pct.jsonl', sample_size=100)
        print(f"âœ… Validation ë°ì´í„° ë¡œë“œë¨: {len(validation_data)}ê°œ ë¬¸ì¥")
    except Exception as e:
        print(f"âŒ Validation ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨: {e}")
        return
    
    # ëª¨ë¸ í‰ê°€
    evaluation = evaluate_model(model_id, validation_data)
    
    # ê²°ê³¼ ì¶œë ¥ ë° ì €ì¥
    print_evaluation_summary(evaluation)
    save_evaluation_results(evaluation)
    
    print("\nâœ… í‰ê°€ ì™„ë£Œ!")

if __name__ == "__main__":
    main() 